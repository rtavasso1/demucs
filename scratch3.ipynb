{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.nonzero(torch.tensor([[0.6, 0.1, 0.0, 0.0],\n",
    "                            [0.0, 0.4, 0.0, 0.0],\n",
    "                            [0.0, 0.0, 1.2, 0.0],\n",
    "                            [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6000,  0.1000,  0.4000,  1.2000, -0.4000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[0.6, 0.1, 0.0, 0.0],\n",
    "                    [0.0, 0.4, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 1.2, 0.0],\n",
    "                    [0.0, 0.0, 0.0,-0.4]])\n",
    "data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor([True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11084\\298101993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#data = data.numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#data = data.numpy()\n",
    "for i, j in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riley\\anaconda3\\envs\\weiner\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Riley\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'import_meta_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46036\\264339979.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Create a saver object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'import_meta_graph'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Path to the checkpoint prefix\n",
    "checkpoint_prefix = './path_to_checkpoint/model.ckpt-569400'\n",
    "\n",
    "# Create a saver object\n",
    "saver = tf.train.import_meta_graph(checkpoint_prefix + '.meta')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the weights from the checkpoint\n",
    "    saver.restore(sess, checkpoint_prefix)\n",
    "    \n",
    "    # Get the default graph\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # Now, use the graph to do things, such as export to SavedModel\n",
    "    # You might need to specify input and output tensors for the model.\n",
    "    # Example:\n",
    "    # inputs = {\"input\": graph.get_tensor_by_name(\"input_tensor_name:0\")}\n",
    "    # outputs = {\"output\": graph.get_tensor_by_name(\"output_tensor_name:0\")}\n",
    "    \n",
    "    # Export the model as a SavedModel\n",
    "    tf.saved_model.simple_save(\n",
    "        session=sess,\n",
    "        export_dir='./exported_saved_model',\n",
    "       # inputs=inputs,\n",
    "        #outputs=outputs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riley\\anaconda3\\envs\\weiner\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Riley\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 The Magenta Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Onset-focused model for piano transcription.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "\n",
    "# from magenta.common import flatten_maybe_padded_sequences\n",
    "# from magenta.common import tf_utils\n",
    "# from magenta.contrib import cudnn_rnn as contrib_cudnn_rnn\n",
    "# from magenta.contrib import rnn as contrib_rnn\n",
    "# from magenta.contrib import training as contrib_training\n",
    "# from magenta.models.onsets_frames_transcription import constants\n",
    "# from magenta.models.onsets_frames_transcription import infer_util\n",
    "# from magenta.models.onsets_frames_transcription import metrics\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1 import estimator as tf_estimator\n",
    "import tf_slim as slim\n",
    "\n",
    "\n",
    "def conv_net(inputs, hparams):\n",
    "  \"\"\"Builds the ConvNet from Kelz 2016.\"\"\"\n",
    "  with slim.arg_scope(\n",
    "      [slim.conv2d, slim.fully_connected],\n",
    "      activation_fn=tf.nn.relu,\n",
    "      weights_initializer=slim.variance_scaling_initializer(\n",
    "          factor=2.0, mode='FAN_AVG', uniform=True)):\n",
    "\n",
    "    net = inputs\n",
    "    i = 0\n",
    "    for (conv_temporal_size, conv_freq_size,\n",
    "         num_filters, freq_pool_size, dropout_amt) in zip(\n",
    "             hparams.temporal_sizes, hparams.freq_sizes, hparams.num_filters,\n",
    "             hparams.pool_sizes, hparams.dropout_keep_amts):\n",
    "      net = slim.conv2d(\n",
    "          net,\n",
    "          num_filters, [conv_temporal_size, conv_freq_size],\n",
    "          scope='conv' + str(i),\n",
    "          normalizer_fn=slim.batch_norm)\n",
    "      if freq_pool_size > 1:\n",
    "        net = slim.max_pool2d(\n",
    "            net, [1, freq_pool_size],\n",
    "            stride=[1, freq_pool_size],\n",
    "            scope='pool' + str(i))\n",
    "      if dropout_amt < 1:\n",
    "        net = slim.dropout(net, dropout_amt, scope='dropout' + str(i))\n",
    "      i += 1\n",
    "\n",
    "    # Flatten while preserving batch and time dimensions.\n",
    "    dims = tf.shape(net)\n",
    "    net = tf.reshape(\n",
    "        net, (dims[0], dims[1], net.shape[2] * net.shape[3]),\n",
    "        'flatten_end')\n",
    "\n",
    "    net = slim.fully_connected(net, hparams.fc_size, scope='fc_end')\n",
    "    net = slim.dropout(net, hparams.fc_dropout_keep_amt, scope='dropout_end')\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def lstm_layer(inputs,\n",
    "               num_units,\n",
    "               lengths=None,\n",
    "               stack_size=1,\n",
    "               use_cudnn=False,\n",
    "               rnn_dropout_drop_amt=0,\n",
    "               bidirectional=True):\n",
    "  \"\"\"Create a LSTM layer using the specified backend.\"\"\"\n",
    "  if use_cudnn:\n",
    "    tf.logging.warning('cuDNN LSTM no longer supported. Using regular LSTM.')\n",
    "  if not bidirectional:\n",
    "    raise ValueError('Only bidirectional LSTMs are supported.')\n",
    "\n",
    "  assert rnn_dropout_drop_amt == 0\n",
    "  cells_fw = [\n",
    "      contrib_cudnn_rnn.CudnnCompatibleLSTMCell(num_units)\n",
    "      for _ in range(stack_size)\n",
    "  ]\n",
    "  cells_bw = [\n",
    "      contrib_cudnn_rnn.CudnnCompatibleLSTMCell(num_units)\n",
    "      for _ in range(stack_size)\n",
    "  ]\n",
    "  with tf.variable_scope('cudnn_lstm'):\n",
    "    (outputs, unused_state_f,\n",
    "     unused_state_b) = contrib_rnn.stack_bidirectional_dynamic_rnn(\n",
    "         cells_fw,\n",
    "         cells_bw,\n",
    "         inputs,\n",
    "         dtype=tf.float32,\n",
    "         sequence_length=lengths,\n",
    "         parallel_iterations=1)\n",
    "\n",
    "  return outputs\n",
    "\n",
    "\n",
    "def acoustic_model(inputs, hparams, lstm_units, lengths):\n",
    "  \"\"\"Acoustic model that handles all specs for a sequence in one window.\"\"\"\n",
    "  conv_output = conv_net(inputs, hparams)\n",
    "\n",
    "  if lstm_units:\n",
    "    return lstm_layer(\n",
    "        conv_output,\n",
    "        lstm_units,\n",
    "        lengths=lengths if hparams.use_lengths else None,\n",
    "        stack_size=hparams.acoustic_rnn_stack_size,\n",
    "        use_cudnn=hparams.use_cudnn,\n",
    "        bidirectional=hparams.bidirectional)\n",
    "\n",
    "  else:\n",
    "    return conv_output\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params, config):\n",
    "  \"\"\"Builds the acoustic model.\"\"\"\n",
    "  del config\n",
    "  hparams = params\n",
    "\n",
    "  length = features.length\n",
    "  spec = features.spec\n",
    "\n",
    "  is_training = mode == tf_estimator.ModeKeys.TRAIN\n",
    "\n",
    "  if is_training:\n",
    "    onset_labels = labels.onsets\n",
    "    offset_labels = labels.offsets\n",
    "    velocity_labels = labels.velocities\n",
    "    frame_labels = labels.labels\n",
    "    frame_label_weights = labels.label_weights\n",
    "\n",
    "  if hparams.stop_activation_gradient and not hparams.activation_loss:\n",
    "    raise ValueError(\n",
    "        'If stop_activation_gradient is true, activation_loss must be true.')\n",
    "\n",
    "  losses = {}\n",
    "  with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "    with tf.variable_scope('onsets'):\n",
    "      onset_outputs = acoustic_model(\n",
    "          spec,\n",
    "          hparams,\n",
    "          lstm_units=hparams.onset_lstm_units,\n",
    "          lengths=length)\n",
    "      onset_probs = slim.fully_connected(\n",
    "          onset_outputs,\n",
    "          constants.MIDI_PITCHES,\n",
    "          activation_fn=tf.sigmoid,\n",
    "          scope='onset_probs')\n",
    "\n",
    "      # onset_probs_flat is used during inference.\n",
    "      onset_probs_flat = flatten_maybe_padded_sequences(onset_probs, length)\n",
    "      if is_training:\n",
    "        onset_labels_flat = flatten_maybe_padded_sequences(onset_labels, length)\n",
    "        onset_losses = tf_utils.log_loss(onset_labels_flat, onset_probs_flat)\n",
    "        tf.losses.add_loss(tf.reduce_mean(onset_losses))\n",
    "        losses['onset'] = onset_losses\n",
    "    with tf.variable_scope('offsets'):\n",
    "      offset_outputs = acoustic_model(\n",
    "          spec,\n",
    "          hparams,\n",
    "          lstm_units=hparams.offset_lstm_units,\n",
    "          lengths=length)\n",
    "      offset_probs = slim.fully_connected(\n",
    "          offset_outputs,\n",
    "          constants.MIDI_PITCHES,\n",
    "          activation_fn=tf.sigmoid,\n",
    "          scope='offset_probs')\n",
    "\n",
    "      # offset_probs_flat is used during inference.\n",
    "      offset_probs_flat = flatten_maybe_padded_sequences(offset_probs, length)\n",
    "      if is_training:\n",
    "        offset_labels_flat = flatten_maybe_padded_sequences(\n",
    "            offset_labels, length)\n",
    "        offset_losses = tf_utils.log_loss(offset_labels_flat, offset_probs_flat)\n",
    "        tf.losses.add_loss(tf.reduce_mean(offset_losses))\n",
    "        losses['offset'] = offset_losses\n",
    "    with tf.variable_scope('velocity'):\n",
    "      velocity_outputs = acoustic_model(\n",
    "          spec,\n",
    "          hparams,\n",
    "          lstm_units=hparams.velocity_lstm_units,\n",
    "          lengths=length)\n",
    "      velocity_values = slim.fully_connected(\n",
    "          velocity_outputs,\n",
    "          constants.MIDI_PITCHES,\n",
    "          activation_fn=None,\n",
    "          scope='onset_velocities')\n",
    "\n",
    "      velocity_values_flat = flatten_maybe_padded_sequences(\n",
    "          velocity_values, length)\n",
    "      if is_training:\n",
    "        velocity_labels_flat = flatten_maybe_padded_sequences(\n",
    "            velocity_labels, length)\n",
    "        velocity_loss = tf.reduce_sum(\n",
    "            onset_labels_flat *\n",
    "            tf.square(velocity_labels_flat - velocity_values_flat),\n",
    "            axis=1)\n",
    "        tf.losses.add_loss(tf.reduce_mean(velocity_loss))\n",
    "        losses['velocity'] = velocity_loss\n",
    "\n",
    "    with tf.variable_scope('frame'):\n",
    "      if not hparams.share_conv_features:\n",
    "        # TODO(eriche): this is broken when hparams.frame_lstm_units > 0\n",
    "        activation_outputs = acoustic_model(\n",
    "            spec,\n",
    "            hparams,\n",
    "            lstm_units=hparams.frame_lstm_units,\n",
    "            lengths=length)\n",
    "        activation_probs = slim.fully_connected(\n",
    "            activation_outputs,\n",
    "            constants.MIDI_PITCHES,\n",
    "            activation_fn=tf.sigmoid,\n",
    "            scope='activation_probs')\n",
    "      else:\n",
    "        activation_probs = slim.fully_connected(\n",
    "            onset_outputs,\n",
    "            constants.MIDI_PITCHES,\n",
    "            activation_fn=tf.sigmoid,\n",
    "            scope='activation_probs')\n",
    "\n",
    "      probs = []\n",
    "      if hparams.stop_onset_gradient:\n",
    "        probs.append(tf.stop_gradient(onset_probs))\n",
    "      else:\n",
    "        probs.append(onset_probs)\n",
    "\n",
    "      if hparams.stop_activation_gradient:\n",
    "        probs.append(tf.stop_gradient(activation_probs))\n",
    "      else:\n",
    "        probs.append(activation_probs)\n",
    "\n",
    "      if hparams.stop_offset_gradient:\n",
    "        probs.append(tf.stop_gradient(offset_probs))\n",
    "      else:\n",
    "        probs.append(offset_probs)\n",
    "\n",
    "      combined_probs = tf.concat(probs, 2)\n",
    "\n",
    "      if hparams.combined_lstm_units > 0:\n",
    "        outputs = lstm_layer(\n",
    "            combined_probs,\n",
    "            hparams.combined_lstm_units,\n",
    "            lengths=length if hparams.use_lengths else None,\n",
    "            stack_size=hparams.combined_rnn_stack_size,\n",
    "            use_cudnn=hparams.use_cudnn,\n",
    "            bidirectional=hparams.bidirectional)\n",
    "      else:\n",
    "        outputs = combined_probs\n",
    "\n",
    "      frame_probs = slim.fully_connected(\n",
    "          outputs,\n",
    "          constants.MIDI_PITCHES,\n",
    "          activation_fn=tf.sigmoid,\n",
    "          scope='frame_probs')\n",
    "\n",
    "    frame_probs_flat = flatten_maybe_padded_sequences(frame_probs, length)\n",
    "\n",
    "    if is_training:\n",
    "      frame_labels_flat = flatten_maybe_padded_sequences(frame_labels, length)\n",
    "      frame_label_weights_flat = flatten_maybe_padded_sequences(\n",
    "          frame_label_weights, length)\n",
    "      if hparams.weight_frame_and_activation_loss:\n",
    "        frame_loss_weights = frame_label_weights_flat\n",
    "      else:\n",
    "        frame_loss_weights = None\n",
    "      frame_losses = tf_utils.log_loss(\n",
    "          frame_labels_flat, frame_probs_flat, weights=frame_loss_weights)\n",
    "      tf.losses.add_loss(tf.reduce_mean(frame_losses))\n",
    "      losses['frame'] = frame_losses\n",
    "\n",
    "      if hparams.activation_loss:\n",
    "        if hparams.weight_frame_and_activation_loss:\n",
    "          activation_loss_weights = frame_label_weights\n",
    "        else:\n",
    "          activation_loss_weights = None\n",
    "        activation_losses = tf_utils.log_loss(\n",
    "            frame_labels_flat,\n",
    "            flatten_maybe_padded_sequences(activation_probs, length),\n",
    "            weights=activation_loss_weights)\n",
    "        tf.losses.add_loss(tf.reduce_mean(activation_losses))\n",
    "        losses['activation'] = activation_losses\n",
    "\n",
    "  frame_predictions = frame_probs_flat > hparams.predict_frame_threshold\n",
    "  onset_predictions = onset_probs_flat > hparams.predict_onset_threshold\n",
    "  offset_predictions = offset_probs_flat > hparams.predict_offset_threshold\n",
    "\n",
    "  frame_predictions = tf.expand_dims(frame_predictions, axis=0)\n",
    "  onset_predictions = tf.expand_dims(onset_predictions, axis=0)\n",
    "  offset_predictions = tf.expand_dims(offset_predictions, axis=0)\n",
    "  velocity_values = tf.expand_dims(velocity_values_flat, axis=0)\n",
    "\n",
    "  metrics_values = metrics.define_metrics(\n",
    "      frame_probs=frame_probs,\n",
    "      onset_probs=onset_probs,\n",
    "      frame_predictions=frame_predictions,\n",
    "      onset_predictions=onset_predictions,\n",
    "      offset_predictions=offset_predictions,\n",
    "      velocity_values=velocity_values,\n",
    "      length=features.length,\n",
    "      sequence_label=labels.note_sequence,\n",
    "      frame_labels=labels.labels,\n",
    "      sequence_id=features.sequence_id,\n",
    "      hparams=hparams)\n",
    "\n",
    "  for label, loss_collection in losses.items():\n",
    "    loss_label = 'losses/' + label\n",
    "    metrics_values[loss_label] = loss_collection\n",
    "\n",
    "  def predict_sequence():\n",
    "    \"\"\"Convert frame predictions into a sequence (TF).\"\"\"\n",
    "\n",
    "    def _predict(frame_probs, onset_probs, frame_predictions, onset_predictions,\n",
    "                 offset_predictions, velocity_values):\n",
    "      \"\"\"Convert frame predictions into a sequence (Python).\"\"\"\n",
    "      sequence = infer_util.predict_sequence(\n",
    "          frame_probs=frame_probs,\n",
    "          onset_probs=onset_probs,\n",
    "          frame_predictions=frame_predictions,\n",
    "          onset_predictions=onset_predictions,\n",
    "          offset_predictions=offset_predictions,\n",
    "          velocity_values=velocity_values,\n",
    "          hparams=hparams,\n",
    "          min_pitch=constants.MIN_MIDI_PITCH)\n",
    "      return sequence.SerializeToString()\n",
    "\n",
    "    sequence = tf.py_func(\n",
    "        _predict,\n",
    "        inp=[\n",
    "            frame_probs[0],\n",
    "            onset_probs[0],\n",
    "            frame_predictions[0],\n",
    "            onset_predictions[0],\n",
    "            offset_predictions[0],\n",
    "            velocity_values[0],\n",
    "        ],\n",
    "        Tout=tf.string,\n",
    "        stateful=False)\n",
    "    sequence.set_shape([])\n",
    "    return tf.expand_dims(sequence, axis=0)\n",
    "\n",
    "  predictions = {\n",
    "      'frame_probs': frame_probs,\n",
    "      'onset_probs': onset_probs,\n",
    "      'frame_predictions': frame_predictions,\n",
    "      'onset_predictions': onset_predictions,\n",
    "      'offset_predictions': offset_predictions,\n",
    "      'velocity_values': velocity_values,\n",
    "      'sequence_predictions': predict_sequence(),\n",
    "      # Include some features and labels in output because Estimator 'predict'\n",
    "      # API does not give access to them.\n",
    "      'sequence_ids': features.sequence_id,\n",
    "      'sequence_labels': labels.note_sequence,\n",
    "      'frame_labels': labels.labels,\n",
    "      'onset_labels': labels.onsets,\n",
    "  }\n",
    "  for k, v in metrics_values.items():\n",
    "    predictions[k] = tf.stack(v)\n",
    "\n",
    "  metric_ops = {k: tf.metrics.mean(v) for k, v in metrics_values.items()}\n",
    "\n",
    "  train_op = None\n",
    "  loss = None\n",
    "  if is_training:\n",
    "    # Creates a pianoroll labels in red and probs in green [minibatch, 88]\n",
    "    images = {}\n",
    "    onset_pianorolls = tf.concat([\n",
    "        onset_labels[:, :, :, tf.newaxis], onset_probs[:, :, :, tf.newaxis],\n",
    "        tf.zeros(tf.shape(onset_labels))[:, :, :, tf.newaxis]\n",
    "    ],\n",
    "                                 axis=3)\n",
    "    images['OnsetPianorolls'] = onset_pianorolls\n",
    "    offset_pianorolls = tf.concat([\n",
    "        offset_labels[:, :, :, tf.newaxis], offset_probs[:, :, :, tf.newaxis],\n",
    "        tf.zeros(tf.shape(offset_labels))[:, :, :, tf.newaxis]\n",
    "    ],\n",
    "                                  axis=3)\n",
    "    images['OffsetPianorolls'] = offset_pianorolls\n",
    "    activation_pianorolls = tf.concat([\n",
    "        frame_labels[:, :, :, tf.newaxis], frame_probs[:, :, :, tf.newaxis],\n",
    "        tf.zeros(tf.shape(frame_labels))[:, :, :, tf.newaxis]\n",
    "    ],\n",
    "                                      axis=3)\n",
    "    images['ActivationPianorolls'] = activation_pianorolls\n",
    "    for name, image in images.items():\n",
    "      tf.summary.image(name, image)\n",
    "\n",
    "    loss = tf.losses.get_total_loss()\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    for label, loss_collection in losses.items():\n",
    "      loss_label = 'losses/' + label\n",
    "      tf.summary.scalar(loss_label, tf.reduce_mean(loss_collection))\n",
    "\n",
    "    train_op = slim.optimize_loss(\n",
    "        name='training',\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_or_create_global_step(),\n",
    "        learning_rate=hparams.learning_rate,\n",
    "        learning_rate_decay_fn=functools.partial(\n",
    "            tf.train.exponential_decay,\n",
    "            decay_steps=hparams.decay_steps,\n",
    "            decay_rate=hparams.decay_rate,\n",
    "            staircase=True),\n",
    "        clip_gradients=hparams.clip_norm,\n",
    "        optimizer='Adam')\n",
    "\n",
    "  return tf_estimator.EstimatorSpec(\n",
    "      mode=mode, predictions=predictions, loss=loss, train_op=train_op,\n",
    "      eval_metric_ops=metric_ops)\n",
    "\n",
    "\n",
    "def get_default_hparams():\n",
    "  \"\"\"Returns the default hyperparameters.\n",
    "\n",
    "  Returns:\n",
    "    A tf.contrib.training.HParams object representing the default\n",
    "    hyperparameters for the model.\n",
    "  \"\"\"\n",
    "  return contrib_training.HParams(\n",
    "      batch_size=8,\n",
    "      learning_rate=0.0006,\n",
    "      decay_steps=10000,\n",
    "      decay_rate=0.98,\n",
    "      clip_norm=3.0,\n",
    "      transform_audio=True,\n",
    "      onset_lstm_units=256,\n",
    "      offset_lstm_units=256,\n",
    "      velocity_lstm_units=0,\n",
    "      frame_lstm_units=0,\n",
    "      combined_lstm_units=256,\n",
    "      acoustic_rnn_stack_size=1,\n",
    "      combined_rnn_stack_size=1,\n",
    "      activation_loss=False,\n",
    "      stop_activation_gradient=False,\n",
    "      stop_onset_gradient=True,\n",
    "      stop_offset_gradient=True,\n",
    "      weight_frame_and_activation_loss=False,\n",
    "      share_conv_features=False,\n",
    "      temporal_sizes=[3, 3, 3],\n",
    "      freq_sizes=[3, 3, 3],\n",
    "      num_filters=[48, 48, 96],\n",
    "      pool_sizes=[1, 2, 2],\n",
    "      dropout_keep_amts=[1.0, 0.75, 0.75],\n",
    "      fc_size=768,\n",
    "      fc_dropout_keep_amt=0.5,\n",
    "      use_lengths=False,\n",
    "      use_cudnn=False,  # DEPRECATED\n",
    "      rnn_dropout_drop_amt=0.0,\n",
    "      bidirectional=True,\n",
    "      predict_frame_threshold=0.5,\n",
    "      predict_onset_threshold=0.5,\n",
    "      predict_offset_threshold=0,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weiner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
